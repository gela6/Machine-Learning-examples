# -*- coding: utf-8 -*-
"""Exercise_insurance_example.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Jj7lHMSIvnmGqBOmQ0MRChqYwBofzeb-
"""

# Import required libraries
import tensorflow as tf
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# Read in the insurance dataset
insurance = pd.read_csv("https://raw.githubusercontent.com/stedy/Machine-Learning-with-R-datasets/master/insurance.csv")
insurance

"""## Preprocessing data (normalization and standardization)"""

from sklearn.compose import make_column_transformer
from sklearn.preprocessing import MinMaxScaler, OneHotEncoder
from sklearn.model_selection import train_test_split

# Create a column transformer
ct = make_column_transformer(
    (MinMaxScaler(), ["age", "bmi", "children"]), # turn all values in these columns between 0 and 1
    (OneHotEncoder(handle_unknown="ignore"), ["sex", "smoker", "region"])
)

# Create X & y values
X = insurance.drop("charges", axis=1)
y = insurance["charges"]

# Build our train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Fit the column transformer to our training data
ct.fit(X_train)

# Transform training and test data with normalization (MinMaxScalar) and OneHotEncoder
X_train_normal = ct.transform(X_train)
X_test_normal = ct.transform(X_test)

"""Building a neural network model"""

tf.random.set_seed(42)

# 1. Create the model
insurance_model_1 = tf.keras.Sequential([
  tf.keras.layers.Dense(100),
  tf.keras.layers.Dense(10),
  tf.keras.layers.Dense(1)
])

# 2. Compile the model
insurance_model_1.compile(loss=tf.keras.losses.mae,
                                   optimizer=tf.keras.optimizers.Adam(),
                                   metrics=["mae"])

# 3. Fit the model
insurance_model_1.fit(X_train_normal, y_train, epochs=200)

# Evaluate our insurance model trained on normalized data
insurance_model_1.evaluate(X_test_normal, y_test)

"""We need a better prediction. Let's try optimize our model."""

tf.random.set_seed(42)

# 1. Create the model
insurance_model_2 = tf.keras.Sequential([
  tf.keras.layers.Dense(100),
  tf.keras.layers.Dense(50),
  tf.keras.layers.Dense(10),
  tf.keras.layers.Dense(1)
])

# 2. Compile the model
insurance_model_2.compile(loss=tf.keras.losses.mae,
                                   optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),
                                   metrics=["mae"])

# 3. Fit the model
history = insurance_model_2.fit(X_train_normal, y_train, epochs=200, verbose=1)

# Evaluate our insurance model trained on normalized data
insurance_model_2.evaluate(X_test_normal, y_test)

# Plot history (also known as a loss curve or a training curve)
pd.DataFrame(history.history).plot()
plt.ylabel("loss")
plt.xlabel("epochs")

"""So we can get this result with ~10 epochs"""